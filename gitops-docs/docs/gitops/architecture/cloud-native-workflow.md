# Cloud-Native Deployment Workflow

## Overview

This document describes the **best-in-class cloud-native deployment workflow** for Super Fortnight microservices, implementing industry-leading GitOps practices with a **Helm + Kustomize hybrid** approach.

## Architecture Principles

### Design Goals

âœ… **Team Autonomy** - Each team owns their service completely  
âœ… **DRY Principle** - Eliminate duplication through templating  
âœ… **GitOps Native** - Git as single source of truth  
âœ… **Environment Parity** - Consistent deployments across environments  
âœ… **Selective Adoption** - Teams control when to adopt platform updates

### Three-Tier Architecture

```mermaid
graph TB
    A[Platform Chart Repository<br/>sf-helm-registry] -->|Kustomize fetches| B[Team Service Repository<br/>aggregator-service]
    B -->|ArgoCD monitors| C[GitOps Repository<br/>gitops-v2]
    C -->|Deploys to| D[Kubernetes Cluster]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#f3e5f5
```

## Repository Structure

### 1. Platform Chart Repository (sf-helm-registry)

**Owner**: Platform Team  
**Purpose**: Centralized Helm chart templates  
**GitHub**: https://github.com/ashutosh-18k92/sf-helm-registry.git

```
sf-helm-registry/
â””â”€â”€ api/
    â”œâ”€â”€ Chart.yaml (v0.1.0)
    â”œâ”€â”€ templates/
    â”‚   â”œâ”€â”€ deployment.yaml      # Deployment with affinity
    â”‚   â”œâ”€â”€ service.yaml          # ClusterIP service
    â”‚   â”œâ”€â”€ hpa.yaml              # Horizontal Pod Autoscaler
    â”‚   â”œâ”€â”€ istioVirtualService.yaml
    â”‚   â””â”€â”€ serviceAccount.yaml
    â””â”€â”€ values.yaml               # Base defaults
```

**Features**:

- Production-ready templates
- Affinity rules for HA
- HPA with CPU/memory targets
- Istio service mesh integration
- Standardized labels and annotations

### 2. Team Service Repository (aggregator-service)

**Owner**: Aggregator Team  
**Purpose**: Application code + deployment configuration  
**GitHub**: https://github.com/ashutosh-18k92/aggregator-service.git

```
aggregator-service/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ index.ts              # TypeScript/Express application
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml  # References sf-helm-registry
â”‚   â”‚   â””â”€â”€ values.yaml         # Service-specific (30 lines)
â”‚   â””â”€â”€ overlays/
â”‚       â”œâ”€â”€ dev/
â”‚       â”‚   â””â”€â”€ kustomization.yaml
â”‚       â””â”€â”€ production/
â”‚           â”œâ”€â”€ kustomization.yaml
â”‚           â””â”€â”€ patches/
â”‚               â”œâ”€â”€ deployment-affinity.yaml
â”‚               â”œâ”€â”€ hpa-scaling.yaml
â”‚               â””â”€â”€ production-resources.yaml
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

**Key Principle**: Only 30 lines of service-specific configuration!

### 3. GitOps Repository (gitops-v2)

**Owner**: Platform Team  
**Purpose**: ArgoCD application definitions

```
gitops-v2/
â””â”€â”€ services/
    â”œâ”€â”€ aggregator-service.yaml      # ArgoCD App (production)
    â”œâ”€â”€ aggregator-service-dev.yaml  # ArgoCD App (dev)
    â””â”€â”€ README.md
```

## Workflow Deep Dive

### Base Chart Configuration

**File**: `aggregator-service/deploy/base/kustomization.yaml`

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

helmCharts:
  - name: api
    repo: https://github.com/ashutosh-18k92/sf-helm-registry.git
    releaseName: aggregator
    namespace: super-fortnight
    valuesFile: values.yaml
    version: 0.1.0 # Team controls version!
    includeCRDs: false
```

**File**: `aggregator-service/deploy/base/values.yaml`

```yaml
# Only service-specific overrides (30 lines total)
containerPort: 3000
image:
  repository: "aggregator-service"
env:
  SERVICE_NAME: "aggregator-service"
virtualService:
  hosts:
    - aggregator
healthCheck:
  livenessProbe:
    httpGet:
      port: 3000
  readinessProbe:
    httpGet:
      port: 3000
```

### Environment Overlays

**Production Patches**:

1. **Affinity** - Node and zone spreading for HA
2. **HPA** - 5-20 replicas with CPU/memory targets
3. **Resources** - Increased limits for production load

**Development**:

- 1 replica
- Debug logging
- Minimal resources
- Fast iteration

### ArgoCD Integration

**File**: `gitops-v2/services/aggregator-service.yaml`

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: aggregator-service
  namespace: argocd
spec:
  source:
    repoURL: https://github.com/ashutosh-18k92/aggregator-service.git
    targetRevision: main
    path: deploy/overlays/production
    kustomize:
      version: v5.0.0
  destination:
    server: https://kubernetes.default.svc
    namespace: super-fortnight
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

## Team Workflows

### Scenario 1: Application Development

```bash
# Clone team repository
git clone https://github.com/your-org/aggregator-service.git
cd aggregator-service

# Develop feature
vim src/index.ts
npm run dev

# Update deployment config (same PR!)
vim deploy/base/values.yaml
# Update image tag

# Commit both code and config
git add src/ deploy/
git commit -m "Add feature X with deployment config"
git push

# ArgoCD auto-syncs to cluster
```

**Benefits**:

- âœ… Code and deployment in single PR
- âœ… Atomic changes
- âœ… Easy rollback
- âœ… Complete team ownership

### Scenario 2: Adopting Platform Updates

```bash
# Platform team releases API chart v0.2.0
# Team reviews changelog and decides to adopt

cd aggregator-service/deploy/base
vim kustomization.yaml
# Change: version: 0.1.0 â†’ 0.2.0

# Test locally
kustomize build ../overlays/production

# Verify changes
kustomize build ../overlays/production | kubectl diff -f -

# Commit when satisfied
git commit -m "Adopt API chart v0.2.0 - adds new monitoring labels"
git push
```

**Benefits**:

- âœ… Team controls timing
- âœ… Can test before deploying
- âœ… No forced updates
- âœ… Gradual rollout across teams

### Scenario 3: Custom Configuration

```bash
# Add production-specific feature flag
cd deploy/overlays/production

cat > patches/feature-flag.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aggregator-api-v1
spec:
  template:
    spec:
      containers:
        - name: api
          env:
            - name: ENABLE_CACHING
              value: "true"
EOF

vim kustomization.yaml
# Add: - patches/feature-flag.yaml

git commit -m "Enable caching in production"
git push
```

**Benefits**:

- âœ… Environment-specific config
- âœ… No base chart modification
- âœ… Preserved across updates
- âœ… Clear separation of concerns

## Platform Team Workflows

### Releasing Base Chart Updates

```bash
# Clone platform chart repository
git clone https://github.com/ashutosh-18k92/sf-helm-registry.git
cd sf-helm-registry/api

# Add new feature
vim templates/deployment.yaml
# Add Prometheus annotations

# Update version
vim Chart.yaml
# version: 0.1.0 â†’ 0.2.0

# Test
helm lint .
helm template test . --dry-run

# Commit and tag
git add .
git commit -m "v0.2.0: Add Prometheus monitoring annotations"
git tag v0.2.0
git push origin main --tags

# Announce to teams
# Teams adopt when ready!
```

### Announcing Updates

```markdown
ðŸ“¢ **API Chart v0.2.0 Released**

**New Features**:

- Prometheus monitoring annotations
- Improved health check defaults
- Updated resource recommendations

**Breaking Changes**: None

**Upgrade**: Update `version: 0.2.0` in your `deploy/base/kustomization.yaml`

**Documentation**: See CHANGELOG.md
```

## Best Practices

### âœ… DO

**Platform Team**:

- Use semantic versioning for chart releases
- Document changes in CHANGELOG
- Test with real services before releasing
- Announce updates to feature teams
- Maintain backward compatibility

**Feature Teams**:

- Pin specific chart versions
- Keep service values minimal
- Use overlays for environment differences
- Test with `kustomize build` before pushing
- Review platform updates before adopting

### âŒ DON'T

**Platform Team**:

- Don't force teams to upgrade
- Don't break compatibility without major version bump
- Don't add service-specific logic to base chart

**Feature Teams**:

- Don't use `latest` for chart version
- Don't duplicate base chart logic
- Don't hardcode values in patches
- Don't skip testing before deployment

## Benefits Achieved

### Team Autonomy

- âœ… Complete ownership of service
- âœ… Single repository for code + deployment
- âœ… Control over platform updates
- âœ… Independent release cycles

### DRY Principle

- âœ… Base chart eliminates duplication
- âœ… Service values: 30 lines (vs 200+ before)
- âœ… Shared templates across all services
- âœ… Consistent patterns

### GitOps Excellence

- âœ… Git as single source of truth
- âœ… Declarative configuration
- âœ… Automated sync via ArgoCD
- âœ… Easy rollback capability
- âœ… Audit trail via Git history

### Environment Management

- âœ… Base values for all environments
- âœ… Environment-specific overlays
- âœ… Production patches for HA
- âœ… Consistent deployment patterns

## Architecture Comparison

| Aspect             | Traditional      | Our Approach                  |
| ------------------ | ---------------- | ----------------------------- |
| **Manifests**      | Plain YAML       | Helm + Kustomize              |
| **Repository**     | Central monorepo | Team-owned per service        |
| **Duplication**    | High             | None (DRY)                    |
| **Team Control**   | Limited          | Complete                      |
| **Updates**        | Forced           | Selective adoption            |
| **Environments**   | Separate files   | Overlays with patches         |
| **Base Templates** | None             | GitHub repository             |
| **Values**         | Hardcoded        | Templated + minimal overrides |

## Verification

### Test Kustomize Build

```bash
cd aggregator-service
kustomize build deploy/overlays/production
```

### Deploy to Cluster

```bash
# Apply ArgoCD application
kubectl apply -f gitops-v2/services/aggregator-service.yaml

# Verify sync status
argocd app get aggregator-service

# Check deployed resources
kubectl get all -n super-fortnight -l app.kubernetes.io/name=aggregator-api-v1
```

## Summary

This workflow represents **best-in-class cloud-native practices**:

âœ… **Team Autonomy** - Complete ownership  
âœ… **DRY Principle** - Zero duplication  
âœ… **GitOps Native** - Git as source of truth  
âœ… **Selective Adoption** - Teams control updates  
âœ… **Environment Parity** - Consistent deployments  
âœ… **Production Ready** - HA, scaling, monitoring

**Result**: Scalable, maintainable, team-friendly deployment workflow! ðŸš€
